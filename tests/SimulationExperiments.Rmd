---
title: "Simulation Experiments"
output: pdf_document
header-includes:
   - \usepackage{booktabs}
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, messages=FALSE)
library(NetMix)
library(tidyverse)
library(RColorBrewer)
library(AUC)
source("NetGenerator.R")
```


# Experiment 1: Static network without covariates. 
Our first simulation exercise estimates the original mixed-membership stochastic block model of Airoldi et al. (2008): a model without covariates and without dynamics. We simulate a single network of 100 nodes and 3 latent blocs using the model's DGP. We set the blocmodel parameters to reflect high in-group interaction probabilities and low out-group interaction probabilities, and generate mixed-memberships that are roughly uniform. The following figure displays the mixed-membership vectors for all 100 nodes in our simulated network. While some nodes are clearly in one of the three blocks (and are therefore depicted in one of the vertices of the simplex), many spread their memberships much more evenly among two or three blocs.       

```{r,fig.align='center', fig.height=3.5, fig.width=5.5}
set.seed(831213)
net1 <-  NetSim(BLK = 3
                ,NODE = 100
                ,STATE = 1
                ,TIME = 1 
                ,DIRECTED = TRUE
                ,N_PRED=0
                ,B_t = matrix(c(5, rep(-5, 3), 5, rep(-5, 3), 5), 
                              ncol=3)
                ,beta_arr = array(c(-1.5, -1.5, -1.5),
                                 c(1, 3, 1))
                ,gamma_vec = c(0))
train_index <- sample(1:10000, 9000)
vcd::ternaryplot(net1$pi_vecs[[1]], col=rgb(1,0,0,0.7), dimnames = 1:3, cex=0.7,
                 main = "Mixed-membership\nNetwork I")
```

We estimate our model using a a k-means initialization streategy, and set special priors for the the blockmodel means that reflect the known DGP. All other hyper-parameters are left at their default values. 

```{r, echo = TRUE}
set.seed(831213)
strt <- proc.time()
net1.model <- mmsbm(formula.dyad = Y ~ 1,
                    senderID = "sender",
                    receiverID = "receiver",
                    nodeID = "node",
                    data.dyad = net1$dyad.data[train_index,],
                    n.blocks = net1$BLK,
                    directed = net1$DIRECTED,
                    mmsbm.control = list(mu_b = c(5,-5),
                                         init = "kmeans"))
our_time <- proc.time() - strt
```

After using a common implementation of the Hungarian algorithm to resolve the allocation problem raised by label switching, we compare known and estimated mixed-membership vectors for all 100 nodes in Figure 2 below. Alignment with the 1:1 diagonal indicates almost perfect retrieval of truth.  
```{r net1_est, fig.align='center', fig.height=3, fig.width=4.5}
loss.mat.net1 <- apply((net1.model$MixedMembership), 1,
                               function(vec){
                                 colSums(((net1$pi_vecs[[1]]) - vec)^2)
                               })
net1_order <- clue::solve_LSAP((loss.mat.net1))
pred_data_static <- data.frame(Network = rep(c("Network I"), each=300),
                               Group = factor(rep(c(1:3), each = 100, times = 1)),
                               Truth = c(c(net1$pi_vecs[[1]])),
                               Pred = c(c(t(net1.model$MixedMembership[net1_order,]))))
ggplot(pred_data_static, aes(x=Truth, y=Pred, color=Group))+
  scale_color_brewer(palette="Set1") + 
  geom_point(alpha=0.7) + 
  facet_wrap(~Network) + 
  theme_bw() + 
  ylab("Estimate") +
  xlab("True mixed-membership")
```

We can also compare our results to those of a fully Bayesian implementation of the MMSBM (viz. the one in package `lda`, which uses a fast collapsed Gibbs sampler to obtain posterior samples of both mixed-membership vectors and the blockmodel). Since our implementation allows for parallelization, we expect its performance to be better than that of a `lda`'s single-chain Gibbs sampler, which cannot be parallelized.  
```{r auc-comp, results='hide'}
##US
test_net1 <- net1$dyad.data[-train_index,] 
test_pi_send <- net1.model$MixedMembership[net1_order,test_net1[,"sender"]]
test_pi_rec <- net1.model$MixedMembership[net1_order,test_net1[,"receiver"]]
test_theta <- plogis(diag(t(test_pi_send)%*%net1.model$BlockModel[net1_order, net1_order]%*%test_pi_rec))
our_auc <- auc((roc(test_theta, as.factor(test_net1$Y))))
##LDA
sociomat <- array(NA, c(100, 100))
sociomat[as.matrix(net1$dyad.data[train_index,c("sender", "receiver")])] <- net1$dyad.data[train_index, "Y"]
sociomat[is.na(sociomat)] <- sample(0:1, sum(is.na(sociomat)), TRUE)
set.seed(831213)
strt <- proc.time()
net1.lda <- lda::mmsb.collapsed.gibbs.sampler(sociomat,
                                              K = 3,
                                              alpha = 0.5,
                                              beta.prior = list(diag(5, 3, 3) + 1,
                                                                diag(-5, 3, 3) + 6),
                                              burnin = 8000L,
                                              num.iterations = 9000L)
lda_time <- proc.time() - strt
loss.mat.lda <- apply(prop.table(net1.lda$document_expects, 2), 1,
                      function(vec){
                        colSums(((net1$pi_vecs[[1]]) - vec)^2)
                      })
lda_order <- clue::solve_LSAP((loss.mat.lda))
bm <- net1.lda$blocks.pos/(net1.lda$blocks.pos+net1.lda$blocks.neg)
lda_pi_send <- prop.table(net1.lda$document_expects,2)[lda_order,test_net1[,"sender"]]
lda_pi_rec <- prop.table(net1.lda$document_expects,2)[lda_order,test_net1[,"receiver"]]
lda_auc <- auc(roc(diag(t(lda_pi_send)%*%bm[lda_order,lda_order]%*%lda_pi_rec), as.factor(test_net1$Y)))
```
Whereas our implementation takes around `r round(our_time["elapsed"], 2)` seconds to estimate the model's parameters using 4 parallel threads, taking 9000 samples from the model's collapsed posterior takes about `r round(lda_time["elapsed"], 2)` seconds on the same computer -- over `r round(lda_time["elapsed"]/our_time["elapsed"])` times as long. Despite the speedup, both implementations produce virtually indistinguishable AUROC's on a held-out random sample of 1000 edges: `r round(our_auc, 3)` and `r round(lda_auc, 3)`, respectively.    


# Experiment 2: Static network with covariates

Our second set of simulations illustrates our model's ability to define a regression model for the mixed-membership vector. We generate 50 networks of 300 nodes and 3 latent blocs, generating mixed-membership vectors as a function of a hypothetical binary treatment assigment. Specifically, the treatment makes nodes much more likely to be part of bloc 2 and less likely to be part of either of the other two blocs. Figure 3 below shows a typical mixed-membership profile for these 50 networks, showing treated nodes as black circles and control nodes as red triangles.  
```{r,fig.align='center', fig.height=3.5, fig.width=5.5}
set.seed(831213)
true_beta <- array(c(0.55, 0.75, 0,
                     -0.5, 1.95, 0,
                     0.75, -0.75, 0),
                   c(3, 3, 1))
net2_list <-  replicate(50, 
                        NetSim(BLK = 3
                               ,NODE = 150
                               ,STATE = 1
                               ,TIME = 1 
                               ,DIRECTED = TRUE
                               ,N_PRED = 2
                               ,B_t = matrix(c(5, rep(-5, 3), 5, rep(-5, 3), 5), 
                                             ncol=3)
                               ,beta_arr = true_beta
                               ,gamma_vec = c(0, 0)),
                        simplify = FALSE)
vcd::ternaryplot(net2_list[[1]]$pi_vecs[[1]],
                 col=rgb(c(1,0),c(0,0),c(0,0),0.7)[net2_list[[1]]$monad.data$V1+1],
                 pch=c(17,19)[net2_list[[1]]$monad.data$V1+1],
                 cex=0.7,
                 main = "Mixed-membership\nNetwork II",
                 dimnames_position = "none"
)
vcd::grid_legend(x = 0.3, y = 0.7,pch=c(17,19),labels = c("Control", "Treated"), col = c("red", "black"), title="Treatment status", frame =FALSE)
```

## Retrieve effect estimates 
We first obtain effect estimates for each of the 50 networks, and plot estimates for the treatment effect for each group of using boxplots in the figure below. We also show the known value of the treatment effect. In all cases, the treatment effect is correctly identified, although our ridge penalty, introduced to avoid early divergence of coefficient estimates, results in attenuation bias.    
```{r, message=FALSE, results='hide'}
set.seed(831213)
system.time(net2.models <- lapply(1:50,
                      function(i){res <- mmsbm(formula.dyad = Y ~ 1,
                              formula.monad= ~ V1,
                              senderID = "sender",
                              receiverID = "receiver",
                              nodeID = "node",
                              data.dyad = net2_list[[i]]$dyad.data,
                              data.monad = net2_list[[i]]$monad.data,
                              n.blocks = net2_list[[i]]$BLK,
                              directed = net2_list[[i]]$DIRECTED,
                              mmsbm.control = list(mu_b = c(5,-5),
                                                   init = "kmeans",
                                                   lda_iter = 5000,
                                                   var_beta = 1,
                                                   em_iter = 5000,
                                                   threads = 22))
                        loss.mat <- apply((res$MixedMembership), 1,
                               function(vec){
                                 colSums(((net2_list[[i]]$pi_vecs[[1]]) - vec)^2)
                               })
                        ord <- clue::solve_LSAP((loss.mat))
                        print(paste("iter:",i))
                        list(res = res, ord = ord)
                        }
                      ))
```
```{r,fig.align='center', fig.height=3, fig.width=4.5, warning=FALSE, message=FALSE}
est_fx <- lapply(1:50,
                 function(i){
                   x <- net2.models[[i]]
                   data.frame(var = rep(c("V1"), 3),
                              grp = rep(c("1","2","3"), each = 1),
                              val = c(x$res$MonadCoef[-1,x$ord,1]),
                              iter = i)
                 }) 
est_fx_df <- do.call(rbind, est_fx)
ggplot(est_fx_df, aes(x=grp, y=val)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(data = data.frame(x=rep(c("1","2","3"), each = 1),
                               y=true_beta[2,,1]),
             aes(x=x, y=y),
             color = "red",
             cex = 2.5) +
  scale_y_continuous(limits = c(min(true_beta[2,,1]),max(true_beta[2,,1])))+
  theme_bw() + 
  ylab("Estimated Posterior Median of Treatment Effect") +
  xlab("Group")
```
## Correct classification
Next, we consider the predictive accuracy of the model with covariates. We compute the area under the ROC curve for a held-out set of 1000 edges. The accuracy measure is then compared to that of a model estimated on the same data but excluding information on the binary treatment variable. We find that, on average, predictive accuracy is higher for the model that incorporates the treatment covarite information.   
```{r, results='hide'}
set.seed(831213)
system.time(net2.models.ridge <- lapply(1:50,
                      function(i){
                        test_index <- sample(1:nrow(net2_list[[i]]$dyad.data),
                                              1000, FALSE)
                        
                        res1 <- mmsbm(formula.dyad = Y ~ 1,
                              formula.monad= ~ V1,
                              senderID = "sender",
                              receiverID = "receiver",
                              nodeID = "node",
                              data.dyad = net2_list[[i]]$dyad.data[-test_index,],
                              data.monad = net2_list[[i]]$monad.data,
                              n.blocks = net2_list[[i]]$BLK,
                              directed = net2_list[[i]]$DIRECTED,
                              mmsbm.control = list(mu_b = c(5,-5),
                                                   init = "kmeans",
                                                   var_beta = 1,
                                                   threads = 22))
                        res2 <- mmsbm(formula.dyad = Y ~ 1,
                              formula.monad= ~ 1,
                              senderID = "sender",
                              receiverID = "receiver",
                              nodeID = "node",
                              data.dyad = net2_list[[i]]$dyad.data[-test_index,],
                              data.monad = net2_list[[i]]$monad.data,
                              n.blocks = net2_list[[i]]$BLK,
                              directed = net2_list[[i]]$DIRECTED,
                              mmsbm.control = list(mu_b = c(5,-5),
                                                   init = "kmeans",
                                                   var_beta = 1,
                                                   em_iter = 5000,
                                                   threads = 22))

                        loss.mat1 <- apply((res1$MixedMembership), 1,
                               function(vec){
                                 colSums(((net2_list[[i]]$pi_vecs[[1]]) - vec)^2)
                               })
                        loss.mat2 <- apply((res2$MixedMembership), 1,
                               function(vec){
                                 colSums(((net2_list[[i]]$pi_vecs[[1]]) - vec)^2)
                               })
                        
                        ord1 <- clue::solve_LSAP((loss.mat1))
                        ord2 <- clue::solve_LSAP((loss.mat2))
                        test_net <- net2_list[[i]]$dyad.data[test_index,] 
                        test_pi_send1 <- res1$MixedMembership[ord1,test_net[,"sender"]]
                        test_pi_rec1 <- res1$MixedMembership[ord1,test_net[,"receiver"]]
                        test_theta1 <- plogis(diag(t(test_pi_send1)%*%res1$BlockModel[ord1, ord1]%*%test_pi_rec1))
                        our_auc1 <- auc((roc(test_theta1, as.factor(test_net$Y))))
                        test_pi_send2 <- res2$MixedMembership[ord2,test_net[,"sender"]]
                        test_pi_rec2 <- res2$MixedMembership[ord2,test_net[,"receiver"]]
                        test_theta2 <- plogis(diag(t(test_pi_send2)%*%res2$BlockModel[ord2, ord2]%*%test_pi_rec2))
                        our_auc2 <- auc((roc(test_theta2, as.factor(test_net$Y))))
                        print(paste("iter:",i))
                        list(res1 = res1, res2 = res2, ord1 = ord1, ord2 = ord2, auroc1 = our_auc1,auroc2 = our_auc2 )
                        }
                      ))
```
```{r, results='asis'}
library(kableExtra)
auc1 <- median(sapply(net2.models.ridge, function(x)x$auroc1))
auc2 <- median(sapply(net2.models.ridge, function(x)x$auroc2))
knitr::kable(data.frame(Model = c("With covariate", "Without covariate"),
                        AUROC = c(auc1,auc2)),
             format = "latex", booktabs = TRUE, digits = 2,
             align = c("l","c")) %>%
  kable_styling(position = "center")
```


# Experiment 3: Dynamic network with covariates

For our third set of simulations, we illustrate the ability of our model to model the network dynamics using a hidden Markov model for the mixed-membership vectors. We generate a network with 200 nodes, and let it evolve over 9 years according to a forward-moving, 2-state hidden Markov model. During the first stage of this 9-year history, a binary treatment is associated with a higher probability of being part of bloc 2. In turn, the relationship is reversed during the second stage of history. Figure 4 shows the distribution of node-level probabilities of belonging to bloc 2 for each time period (a transition occurs from year 5 to year 6).  
```{r, fig.align='center', fig.width=9, fig.height=6}
set.seed(831213)
net3 <- NetSim(BLK = 3
                ,NODE = 200
                ,STATE = 2
                ,TIME = 9
                ,DIRECTED = TRUE
               ,N_PRED = 2
                ,B_t = matrix(c(5, rep(-5, 3), 5, rep(-5, 3), 5), 
                              ncol=3)
                ,A_orig = matrix(c(0.75, 0, 0.25, 1.0), ncol = 2)
                ,beta_arr = array(c(.5, -1.25, 0,
                                   -.5, 1.25, 0,
                                   .5, -1.25, 0,
                                   -1.2, 1.2, 0,
                                   1.2, -1.2, 0,
                                   -1.2, 1.2, 0),
                                 c(3, 3, 2))
                ,gamma_vec = c(0, 0))

ggplot(net3$monad.data, aes(x=as.factor(V1), y=pi2)) +
  geom_boxplot(outlier.shape = NA) +
  facet_wrap(~year) +
  theme_bw() + 
  xlab("Treatment status") +
  ylab("Probability of being in bloc 2")
```

We compare our model's performance to that of a separate estimation for each time period.
```{r, results='hide'}
set.seed(831213)
net3.model <- mmsbm(formula.dyad = Y ~ 1,
                    formula.monad= ~ V1,
                    senderID = "sender",
                    receiverID = "receiver",
                    nodeID = "node",
                    timeID = "year",
                    data.dyad = net3$dyad.data,
                    data.monad = net3$monad.data,
                    n.blocks = net3$BLK,
                    n.hmmstates = net3$STATE,
                    directed = net3$DIRECTED,
                    mmsbm.control = list(mu_b = c(5,-5),
                                         var_b = c(1, 1),
                                         init = "kmeans",
                                         lda_iter = 50000,
                                         threads = 12
                                         ))
loss.mat.net3 <- apply((net3.model$MixedMembership), 1,
                       function(vec){
                         colSums((do.call(rbind,net3$pi_vecs) - vec)^2)
                       })
net3_order <- clue::solve_LSAP((loss.mat.net3))

```

```{r}
##Separate models for each year
sep_dyaddata <- split(net3$dyad.data, net3$dyad.data$year)
sep_monaddata <- split(net3$monad.data, net3$monad.data$year)
set.seed(831213)
static.models <- lapply(1:9, 
                        function(y){
                          temp_res <- mmsbm(formula.dyad = Y ~ V1,
                                formula.monad= ~ V1,
                                senderID = "sender",
                                receiverID = "receiver",
                                nodeID = "node",
                                data.dyad = sep_dyaddata[[y]],
                                data.monad = sep_monaddata[[y]],
                                n.blocks = net3$BLK,
                                directed = net3$DIRECTED,
                                mmsbm.control = list(mu_b = c(5,-5),
                                                     var_b = c(1, 1),
                                                     init = "kmeans",
                                                     lda_iter = 50000,
                                                     threads = 12))
                          loss.mat <- apply((temp_res$MixedMembership), 1,
                                            function(vec){
                                              colSums(((net3$pi_vecs[[y]]) - vec)^2)
                                            })
                          ord <- clue::solve_LSAP((loss.mat))
                          t(temp_res$MixedMembership[ord, ])
                        })
```
```{r, fig.align='center', fig.width=11, fig.height=6}
net3$monad.data$pred_pi2_full <- net3.model$MixedMembership[net3_order[2],]
net3$monad.data$pred_pi2_sep <- do.call(rbind,static.models)[,2]
long_net3 <- net3$monad.data %>%
  select(year, V1, pi2, pred_pi2_full, pred_pi2_sep) %>%
  gather(Source, val, c(pi2, pred_pi2_full, pred_pi2_sep)) %>%
  mutate(Source = recode(Source, pi2="Truth", pred_pi2_full="Dynamic Model", pred_pi2_sep="Separate Static Models"))
ggplot(long_net3, aes(x=as.factor(V1), y=val, color=Source)) +
  geom_boxplot(outlier.shape = NA) +
  facet_wrap(~year) +
  theme_bw() + 
  xlab("Treatment status") +
  ylab("Probability of being in bloc 2")
```








